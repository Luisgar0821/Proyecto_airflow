# üß¨ ETL Lung Cancer Project with Apache Airflow

Este proyecto implementa un pipeline ETL completo utilizando **Apache Airflow** para procesar, transformar y cargar datos relacionados con el c√°ncer de pulm√≥n en una base de datos **PostgreSQL**, empleando un modelo dimensional tipo **estrella**. El flujo permite an√°lisis eficientes y estructurados a partir de un dataset enriquecido.

---

## üìå Objetivo

Automatizar la extracci√≥n, transformaci√≥n y carga de datos de c√°ncer de pulm√≥n para su posterior an√°lisis, integrando pr√°cticas de ingenier√≠a de datos y modelado dimensional.

---

## üß± Estructura del Proyecto

```
Proyecto_airflow/
‚îÇ
‚îú‚îÄ‚îÄ dags/               # DAG principal de Airflow
‚îÇ   ‚îî‚îÄ‚îÄ etl_lung_cancer.py
‚îú‚îÄ‚îÄ tasks/              # Tareas ETL
‚îÇ   ‚îú‚îÄ‚îÄ extract_task.py
‚îÇ   ‚îú‚îÄ‚îÄ transform.py
‚îÇ   ‚îú‚îÄ‚îÄ load_dimensions.py
‚îÇ   ‚îî‚îÄ‚îÄ load_fact.py
‚îú‚îÄ‚îÄ venv/               # Entorno virtual (incluido en Git)
‚îú‚îÄ‚îÄ requirements.txt    # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md
```

> Nota: aunque Airflow se usa en este proyecto, la carpeta `airflow/` est√° **excluida** en `.gitignore` y no forma parte del repositorio.

---

## ‚öôÔ∏è Pipeline ETL en Airflow

El DAG `etl_lung_cancer` est√° compuesto por las siguientes tareas:

1. **Extracci√≥n (`extract_from_postgres`)**: carga los datos desde una tabla staging.
2. **Transformaci√≥n (`transform_data`)**: limpieza y normalizaci√≥n del dataset.
3. **Carga de dimensiones (`load_dimensions`)**: inserta datos en las tablas dimensionales, evitando duplicados.
4. **Carga de hechos (`load_fact_table`)**: inserta registros con claves for√°neas hacia las dimensiones.

---

## üìÇ Modelo Dimensional

Modelo en estrella con la tabla de hechos `hechos_persona` al centro:

### Tabla de hechos

- `edad`
- `tasa_prevalencia`
- `tasa_mortalidad`
- `survival_years`
- `id_pais`
- `id_salud`
- `id_exposicion`
- `id_fumador`
- `id_diagnostico`
- `id_stage`
- `id_tratamiento`
- `id_historia`

### Tablas dimensionales

- `dim_pais(country, developed_or_developing, annual_deaths)`
- `dim_salud(healthcare_access, early_detection, treatment_type, cancer_stage)`
- `dim_exposicion(air_pollution_exposure, occupational_exposure, indoor_pollution, family_history)`
- `dim_fumador(smoker, passive_smoker, years_of_smoking, cigarettes_per_day)`
- `dim_diagnostico(descripcion)`
- `dim_cancer_stage(stage)`
- `dim_tratamiento(tratamiento)`
- `dim_historia_familiar(descripcion)`

---

## üß™ Requisitos

### Instalaci√≥n de dependencias

Crear y activar un entorno virtual:

```bash
python3 -m venv venv
source venv/bin/activate
```

Instalar los paquetes necesarios:

```bash
pip install -r requirements.txt
```

### Instalaci√≥n de PostgreSQL

Instala PostgreSQL en sistemas Debian/Ubuntu:

```bash
sudo apt install postgresql postgresql-contrib
```

Establecer la contrase√±a del usuario `postgres`:

```bash
sudo -u postgres psql -c "ALTER USER postgres WITH PASSWORD 'admin';"
```

### Instalaci√≥n de Apache Airflow

Dentro del entorno virtual (`venv`) ejecutar:

```bash
pip install apache-airflow
```

---

## ‚ñ∂Ô∏è Ejecuci√≥n

1. Ejecutar el notebook de carga inicial de datos para poblar la tabla staging.
2. Exportar la ruta del proyecto como AIRFLOW_HOME:

```bash
export AIRFLOW_HOME=~/Proyecto_airflow/airflow
```

3. Inicializar el servidor web de Airflow:

```bash
airflow standalone
```

4. Activar el DAG `etl_lung_cancer` desde la interfaz web de Airflow.

---

## ‚ùå Consideraciones

- La base de datos debe estar en PostgreSQL local (`lung_cancer_database`).
- Se recomienda ejecutar el DAG en entorno controlado por `venv/`.
- La carpeta `airflow/` est√° ignorada por Git para evitar subir archivos innecesarios de ejecuci√≥n.

---

## üé¨ Video demostrativo

_Agregue aqu√≠ el enlace a un video donde se muestra el funcionamiento del pipeline:_

```
[Ver video de demostraci√≥n](enlace-al-video)
```

---

## üìå Autor

**Luis Angel Garc√≠a**  
*Estudiante de IA*

**Yalany Willy Cardenas**  
*Estudiante de IA*

**Juliana Toro**  
*Estudiante de IA*

